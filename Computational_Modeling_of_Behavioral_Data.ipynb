{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Computational Modeling of Behavioral Data by Prof. Kentaro Katahira\n\n## Rescorla-Wagner model"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Plots\nusing Interact\nusing Random\n\n\"\"\"\nNâ‚œ: number of trials\nÎ±: learning rate\nPáµ£: probability of getting reward\n\"\"\"\n\n@manipulate for Nâ‚œ = 0:1:500, Î± = 0:0.05:1, Páµ£ = 0:0.05:1\n\n    rng = MersenneTwister(1234) #create a seed for random numbers\n\n    ğ• = zeros(Nâ‚œ) #strengths of association as Nâ‚œ-length vector\n    ğ‘ = rand(rng, Nâ‚œ) .< Páµ£ # presence of reinforcement (1 or 0) as Nâ‚œ-length vector\n\n    for t in 1: Nâ‚œ-1\n\n        ğ•[t+1] = ğ•[t] + Î± *(ğ‘[t]-ğ•[t])\n    end\n\n    plot(ğ•, label= string(\"a \", Î±))\n    plot!([(i, Páµ£) for i in 1:1:Nâ‚œ], label=\"expected value of r: \" * string(Páµ£))\n    xlabel!(\"number of trials\")\n    ylabel!(\"strength of association\")\n    ylims!((0, 1))\n    title!(\"Rescorla-Wagner model\")\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-learning simulation\n### softmax function"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function softmax(Î², Î”q)\n    return 1 / (1+ exp(-Î² * (Î”q)))\nend\n\n@manipulate for Î² in 0:0.05:5\n    plot([(Î”q, softmax(Î², Î”q)) for Î”q in -4:0.1:4], m=:o, label=string(\"beta \", Î²))\n    xlabel!(\"difference in Q\")\n    ylabel!(\"probability\")\n    ylims!((0, 1))\n    title!(\"Softmax Function\")\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### interactive plot of Q-learning model"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"\nNâ‚œ: number of trials\nÎ±: learning rate\nÎ²: inverse temperature\nPáµ£: probability of getting reward in A\n\"\"\"\n\n@manipulate for Nâ‚œ in 0:5:200, Î± in 0:0.05:1, Î² in 0:0.25:5, Páµ£ in 0:0.05:1\n\n    rng = MersenneTwister(1234)\n\n    ğ = zeros((2, Nâ‚œ)) #initial value of Q in 2 by Nâ‚œ matrix\n    ğœ = zeros(Int, Nâ‚œ) #initial choice in each Nâ‚œ trial\n    ğ« = zeros(Nâ‚œ) # 0 (no reward) or 1 (reward) in each Nâ‚œ trial\n    Pâ‚ = zeros(Nâ‚œ) # probability of choosing A in each trial\n    P = (Páµ£, 1-Páµ£)\n\n    for t in 1:Nâ‚œ-1\n        Pâ‚ = softmax(Î², ğ[1, t] - ğ[2, t])\n\n        if rand(rng) < Pâ‚\n            ğœ[t] = 1 #choose A\n            ğ«[t] = Int(rand(rng) < P[1])\n        else\n            ğœ[t] = 2 #choose B\n            ğ«[t] = Int(rand(rng) < P[2])\n        end\n\n        ğ[ğœ[t], t+1] = ğ[ğœ[t], t] + Î± * (ğ«[t] - ğ[ğœ[t], t])\n        ğ[3 - ğœ[t], t+1] = ğ[3 - ğœ[t], t] # retain value of unpicked choice\n    end\n\n    plot(ğ[1, :], label=\"Qt(A)\", color=\"orange\")\n    plot!([(i, P[1]) for i in 1:1:Nâ‚œ], label=\"expected value of reward for A:\" * string(P[1]), color=\"darkorange\")\n    plot!(ğ[2, :], label=\"Qt(B)\", color=\"skyblue\")\n    plot!([(i, P[2]) for i in 1:1:Nâ‚œ], label=\"expected value of reward for B:\" * string(P[2]), color=\"darkblue\")\n    xlabel!(\"number of trials\")\n    ylabel!(\"Q (value of behavior?)\")\n    ylims!((0, 1))\n    title!(\"Q-learning model\")\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameter Estimation of Q-learing model\n\n### Preparation"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function generate_qlearning_data(Nâ‚œ, Î±, Î², Páµ£)\n\n    ğ = zeros((2, Nâ‚œ)) #initial value of Q in 2 by Nâ‚œ matrix\n    ğœ = zeros(Int, Nâ‚œ) #initial choice in each Nâ‚œ trial\n    ğ« = zeros(Nâ‚œ) # 0 (no reward) or 1 (reward) in each Nâ‚œ trial\n    Pâ‚ = zeros(Nâ‚œ) # probability of choosing A in each trial\n    P = (Páµ£, 1-Páµ£)\n\n    for t in 1:Nâ‚œ-1\n        Pâ‚ = softmax(Î², ğ[1, t] - ğ[2, t])\n\n        if rand() < Pâ‚\n            ğœ[t] = 1 #choose A\n            ğ«[t] = (rand() < P[1])\n        else\n            ğœ[t] = 2 #choose B\n            ğ«[t] = Int(rand() < P[2])\n        end\n\n        ğ[ğœ[t], t+1] = ğ[ğœ[t], t] + Î± * (ğ«[t] - ğ[ğœ[t], t])\n        ğ[3 - ğœ[t], t+1] = ğ[3 - ğœ[t], t] # retain value of unpicked choice\n    end\n\n    return ğœ, ğ«\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"\ninit_values: [Î±, Î²]\nÎ±: learning rate\nÎ²: inverse temperature\nğœ: vector of choices in each Nâ‚œ trial in 1(A) or 2(B)\nğ«: 0 (no reward) or 1 (reward) in each Nâ‚œ trial\n\"\"\"\nfunction func_qlearning(init_values, ğœ, ğ«) #needed for passing list as variables for Optim\n\n    Nâ‚œ = length(ğœ)\n    Pâ‚ = zeros(Nâ‚œ) #probabilities of selecting A\n    ğ = zeros(Real, (2, Nâ‚œ))\n    logl = 0 #initial value of log likelihood\n\n    for t in 1:Nâ‚œ - 1\n        Pâ‚[t] = softmax(init_values[2], ğ[1, t] - ğ[2, t])\n        logl += (ğœ[t] == 1) * log(Pâ‚[t]) + (ğœ[t] == 2) * log(1 - Pâ‚[t])\n        ğ[ğœ[t], t + 1] = ğ[ğœ[t], t] + init_values[1] * (ğ«[t] - ğ[ğœ[t], t])\n        ğ[3 - ğœ[t], t + 1] =  ğ[3 - ğœ[t], t]\n    end\n\n    return (negll = -logl, ğ = ğ, Pâ‚ = Pâ‚);\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameter Estimation\n\n### optimization with JuMP and Ipopt"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "#if you get no package error for Ipopt, comment out the follwoing lines and run again\n#import Pkg\n#Pkg.add(\"Pkg\")\n#Pkg.add(\"Ipopt\")\n#Pkg.build(\"Ipopt\")\n\nusing JuMP, Ipopt, ForwardDiff\n\n#@manipulate for Nâ‚œ in 0:50:1000, Î±1 in 0:0.05:1, Î²1 in 0:0.25:5, Páµ£ in 0:0.05:1\n\nğœ, ğ« = generate_qlearning_data(Nâ‚œ, Î±1, Î²1, Páµ£)\nfunc_qlearning_JuMP(Î±, Î²) = func_qlearning((Î±, Î²), ğœ, ğ«).negll #JuMP requires separate arguments, not a list\n\nm = Model(Ipopt.Optimizer)\nregister(m, :func_qlearning_JuMP, 2, func_qlearning_JuMP, autodiff=true)\n\n@variable(m, 0.0 <= Î± <= 1.0, start=rand(), base_name = \"learning_rate\")\n@variable(m, 0.0 <= Î² <= 5.0, start=5*rand(), base_name = \"inverse_temperature\")\n\n@NLobjective(m, Min, func_qlearning_JuMP(Î±, Î²))\noptimize!(m)\nprint(\"\",\" Î± = \", value(Î±), \" Î² = \", value(Î²))\n#end"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### optimization with Optim"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Optim\n\n@manipulate for Nâ‚œ in 0:5:200, Î± in 0:0.05:1, Î² in 0:0.25:5, Páµ£ in 0:0.05:1\n    ğœ, ğ« = generate_qlearning_data(Nâ‚œ, Î±, Î², Páµ£)\n\n    func_qlearning_opt(init_values) = func_qlearning(init_values, ğœ, ğ«).negll\n\n    initial_values = rand(2)\n    lower = [0.0, 0.0]\n    upper = [1.0, 5.0]\n    inner_optimizer = GradientDescent()\n    results = optimize(func_qlearning_opt, lower, upper, initial_values, Fminbox(inner_optimizer))\n    #@show optimize(func_qlearning_opt, init_values, lower, upper, LBFGS())\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### optimization with BlackBoxOptim, which is designed for blackbox functions, so this part is only for demonstration purpose"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using BlackBoxOptim\n\n@manipulate for Nâ‚œ in 0:5:200, Î± in 0:0.05:1, Î² in 0:0.25:5, Páµ£ in 0:0.05:1\n    ğœ, ğ« = generate_qlearning_data(Nâ‚œ, Î±, Î², Páµ£)\n\n    func_qlearning_opt(init_values) = func_qlearning(init_values, ğœ, ğ«).negll\n\n    results = bboptimize(func_qlearning_opt; SearchRange = [(0.0, 1.0), (0.0, 5.0)], NumDimensions = 2);\n    best_candidate(results)\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We can also compare performances when using different optimizers."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "#this cell takes a lot time to run, so execute it only if you want to\n\n#ğœ, ğ« = generate_qlearning_data(100, 0.3, 1.2, 0.5)\n#func_qlearning_opt(init_values) = func_qlearning(init_values, ğœ, ğ«).negll\n#compare_optimizers(func_qlearning_opt; SearchRange = [(0.0, 1.0), (0.0, 5.0)], NumDimensions = 2);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## comparison of models\n\n### win-stay lose-shift (WSLS) model"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"\nNâ‚œ: number of trials\nÏµ: error rate\nPáµ£: probability of getting reward in A\n\"\"\"\nfunction wsls_simulstion(Nâ‚œ, Ïµ, Páµ£, seed=1234)\n\n    rng = MersenneTwister(seed)\n\n    Pâ‚ = zeros(Nâ‚œ) #probabilities of selecting A\n    Pâ‚[1] = 0.5 # probability at initial trial is 0.5\n    ğœ = zeros(Int, Nâ‚œ) #initial choice in each Nâ‚œ trial\n    ğ« = zeros(Nâ‚œ) # 0 (no reward) or 1 (reward) in each Nâ‚œ trial\n\n    for t in 1:Nâ‚œ-1\n\n        chooseAB = rand(rng)\n        get_reward = rand(rng)\n\n        #select A with reward\n        if chooseAB < Pâ‚[t] && get_reward <  Páµ£\n\n            Pâ‚[t + 1] = 1 - Ïµ\n            ğœ[t] = 1\n            ğ«[t] = 1\n\n        #select B with no reward\n        elseif chooseAB > Pâ‚[t] && get_reward >  Páµ£\n\n            Pâ‚[t + 1] = 1 - Ïµ\n            ğœ[t] = 2\n            ğ«[t] = 0\n\n        #select A with no reward\n        elseif chooseAB < Pâ‚[t] && get_reward >  Páµ£\n\n            Pâ‚[t + 1] = Ïµ\n            ğœ[t] = 1\n            ğ«[t] = 0\n        #select B with reward\n        elseif chooseAB > Pâ‚[t] && get_reward <  Páµ£\n\n            Pâ‚[t + 1] = Ïµ\n            ğœ[t] = 2\n            ğ«[t] = 1\n\n        end\n\n    end\n\n    return (Pâ‚ = Pâ‚, ğœ = ğœ, ğ« = ğ«);\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### plot"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@manipulate for Nâ‚œ in 0:5:200, Ïµ in 0:0.05:1, Páµ£ in 0:0.05:1, seed in 1:1:1234\n\n    Pâ‚ = wsls_simulstion(Nâ‚œ, Ïµ, Páµ£, seed).Pâ‚\n\n    plot(Pâ‚, label=\"P(a = A)\", color=\"orange\")\n    ylabel!(\"P(a = A)\")\n    ylims!((0, 1))\n    title!(\"WSLS Model\")\n\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### random selection model"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function random_choice_simulation(Nâ‚œ, Pâ‚, seed=1234)\n\n    rng = MersenneTwister(seed)\n\n    ğœ = 2 .- Int.(rand(rng, Nâ‚œ) .< Pâ‚) #dot notation in Julia signifies elemnet-wise operation\n\n    return (Pâ‚ = Pâ‚, ğœ = ğœ)\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "####plot"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@manipulate for Nâ‚œ in 0:5:200, Pâ‚ in 0:0.05:1\n\n    plot([Pâ‚ for i in range(1, stop=Nâ‚œ)], label=\"P(a = A)\", color=\"orange\")\n    ylabel!(\"P(a = A)\")\n    ylims!((0, 1))\n    title!(\"Random Choice Model\")\n\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### model comparison\n\n#### preparation"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"\nÏµ: error rate\nğœ: vector of choices in each Nâ‚œ trial in 1(A) or 2(B)\nğ«: 0 (no reward) or 1 (reward) in each Nâ‚œ trial\n\nwhen given Ïµ, ğœ, and ğ«, returns log likelihood and Pâ‚\n\"\"\"\nfunction func_wsls(Ïµ, ğœ, ğ«)\n\n    Nâ‚œ = length(ğœ)\n    Pâ‚ = zeros(Nâ‚œ) #probabilities of selecting A\n    Pâ‚[1] = 0.5\n    logl = 0 #initial value of log likelihood\n\n    for t in 1:Nâ‚œ - 1\n        logl += (ğœ[t] == 1) * log(Pâ‚[t]) + (ğœ[t] == 2) * log(1 - Pâ‚[t])\n\n        #select A with reward\n        if ğœ[t] == 1 &&   ğ«[t] == 1\n\n            Pâ‚[t + 1] = 1 - Ïµ\n\n        #select B with no reward\n        elseif  ğœ[t] == 2 &&   ğ«[t] == 0\n\n            Pâ‚[t + 1] = 1 - Ïµ\n\n        #select A with no reward\n        elseif  ğœ[t] == 1 &&   ğ«[t] == 0\n\n            Pâ‚[t + 1] = Ïµ\n\n        #select B with reward\n        elseif ğœ[t] == 2 &&   ğ«[t] == 1\n\n            Pâ‚[t + 1] = Ïµ\n\n        end\n    end\n\n    return (ll = logl, Pâ‚ = Pâ‚);\nend\n\n\n\"\"\"\nPâ‚: probability of choosing A\nğœ: vector of choices in each Nâ‚œ trial in 1(A) or 2(B)\nğ«: 0 (no reward) or 1 (reward) in each Nâ‚œ trial\n\nwhen given Pâ‚, ğœ, and ğ«, returns log likelihood and Pâ‚\n\"\"\"\nfunction func_random_choice(Pâ‚, ğœ, ğ«)\n\n    Nâ‚œ = length(ğœ)\n    logl = 0\n\n    for t in 1:Nâ‚œ\n        logl += (ğœ[t] == 1) * log(Pâ‚) + (ğœ[t] == 2) * log(1 - Pâ‚)\n    end\n\n    return logl\n\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### parameter estimation with JuMP"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using JuMP, Ipopt, ForwardDiff\n\n@manipulate for Nâ‚œ in 0:50:1000, Î±1 in 0:0.05:1, Î²1 in 0:0.25:5, Páµ£ in 0:0.05:1\n\n    ğœ, ğ« = generate_qlearning_data(Nâ‚œ, Î±1, Î²1, Páµ£)\n    func_qlearning_JuMP(Î±, Î²) = func_qlearning((Î±, Î²), ğœ, ğ«).negll #JuMP requires separate arguments, not a list\n\n    m = Model(Ipopt.Optimizer)\n    register(m, :func_qlearning_JuMP, 2, func_qlearning_JuMP, autodiff=true)\n\n    @variable(m, 0.0 <= Î± <= 1.0, start=rand(), base_name = \"learning_rate\")\n    @variable(m, 0.0 <= Î² <= 5.0, start=5*rand(), base_name = \"inverse_temperature\")\n\n    @NLobjective(m, Min, func_qlearning_JuMP(Î±, Î²))\n    optimize!(m)\n    print(\"\",\" Î± = \", value(Î±), \" Î² = \", value(Î²))"
      ],
      "metadata": {},
      "execution_count": null
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.4.1"
    },
    "kernelspec": {
      "name": "julia-1.4",
      "display_name": "Julia 1.4.1",
      "language": "julia"
    }
  },
  "nbformat": 4
}
