{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Computational Modeling of Behavioral Data by Prof. Kentaro Katahira\n\n## Rescorla-Wagner model"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Plots\nusing Interact\nusing Random\n\n\"\"\"\nNₜ: number of trials\nα: learning rate\nPᵣ: probability of getting reward\n\"\"\"\n\n@manipulate for Nₜ = 0:1:500, α = 0:0.05:1, Pᵣ = 0:0.05:1\n\n    rng = MersenneTwister(1234) #create a seed for random numbers\n\n    𝐕 = zeros(Nₜ) #strengths of association as Nₜ-length vector\n    𝐑 = rand(rng, Nₜ) .< Pᵣ # presence of reinforcement (1 or 0) as Nₜ-length vector\n\n    for t in 1: Nₜ-1\n\n        𝐕[t+1] = 𝐕[t] + α *(𝐑[t]-𝐕[t])\n    end\n\n    plot(𝐕, label= string(\"a \", α))\n    plot!([(i, Pᵣ) for i in 1:1:Nₜ], label=\"expected value of r: \" * string(Pᵣ))\n    xlabel!(\"number of trials\")\n    ylabel!(\"strength of association\")\n    ylims!((0, 1))\n    title!(\"Rescorla-Wagner model\")\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-learning simulation\n### softmax function"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function softmax(β, Δq)\n    return 1 / (1+ exp(-β * (Δq)))\nend\n\n@manipulate for β in 0:0.05:5\n    plot([(Δq, softmax(β, Δq)) for Δq in -4:0.1:4], m=:o, label=string(\"beta \", β))\n    xlabel!(\"difference in Q\")\n    ylabel!(\"probability\")\n    ylims!((0, 1))\n    title!(\"Softmax Function\")\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### interactive plot of Q-learning model"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"\nNₜ: number of trials\nα: learning rate\nβ: inverse temperature\nPᵣ: probability of getting reward in A\n\"\"\"\n\n@manipulate for Nₜ in 0:5:200, α in 0:0.05:1, β in 0:0.25:5, Pᵣ in 0:0.05:1\n\n    rng = MersenneTwister(1234)\n\n    𝐐 = zeros((2, Nₜ)) #initial value of Q in 2 by Nₜ matrix\n    𝐜 = zeros(Int, Nₜ) #initial choice in each Nₜ trial\n    𝐫 = zeros(Nₜ) # 0 (no reward) or 1 (reward) in each Nₜ trial\n    Pₐ = zeros(Nₜ) # probability of choosing A in each trial\n    P = (Pᵣ, 1-Pᵣ)\n\n    for t in 1:Nₜ-1\n        Pₐ = softmax(β, 𝐐[1, t] - 𝐐[2, t])\n\n        if rand(rng) < Pₐ\n            𝐜[t] = 1 #choose A\n            𝐫[t] = Int(rand(rng) < P[1])\n        else\n            𝐜[t] = 2 #choose B\n            𝐫[t] = Int(rand(rng) < P[2])\n        end\n\n        𝐐[𝐜[t], t+1] = 𝐐[𝐜[t], t] + α * (𝐫[t] - 𝐐[𝐜[t], t])\n        𝐐[3 - 𝐜[t], t+1] = 𝐐[3 - 𝐜[t], t] # retain value of unpicked choice\n    end\n\n    plot(𝐐[1, :], label=\"Qt(A)\", color=\"orange\")\n    plot!([(i, P[1]) for i in 1:1:Nₜ], label=\"expected value of reward for A:\" * string(P[1]), color=\"darkorange\")\n    plot!(𝐐[2, :], label=\"Qt(B)\", color=\"skyblue\")\n    plot!([(i, P[2]) for i in 1:1:Nₜ], label=\"expected value of reward for B:\" * string(P[2]), color=\"darkblue\")\n    xlabel!(\"number of trials\")\n    ylabel!(\"Q (value of behavior?)\")\n    ylims!((0, 1))\n    title!(\"Q-learning model\")\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameter Estimation of Q-learing model\n\n### Preparation"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function generate_qlearning_data(Nₜ, α, β, Pᵣ)\n\n    𝐐 = zeros((2, Nₜ)) #initial value of Q in 2 by Nₜ matrix\n    𝐜 = zeros(Int, Nₜ) #initial choice in each Nₜ trial\n    𝐫 = zeros(Nₜ) # 0 (no reward) or 1 (reward) in each Nₜ trial\n    Pₐ = zeros(Nₜ) # probability of choosing A in each trial\n    P = (Pᵣ, 1-Pᵣ)\n\n    for t in 1:Nₜ-1\n        Pₐ = softmax(β, 𝐐[1, t] - 𝐐[2, t])\n\n        if rand() < Pₐ\n            𝐜[t] = 1 #choose A\n            𝐫[t] = (rand() < P[1])\n        else\n            𝐜[t] = 2 #choose B\n            𝐫[t] = Int(rand() < P[2])\n        end\n\n        𝐐[𝐜[t], t+1] = 𝐐[𝐜[t], t] + α * (𝐫[t] - 𝐐[𝐜[t], t])\n        𝐐[3 - 𝐜[t], t+1] = 𝐐[3 - 𝐜[t], t] # retain value of unpicked choice\n    end\n\n    return 𝐜, 𝐫\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"\ninit_values: [α, β]\nα: learning rate\nβ: inverse temperature\n𝐜: vector of choices in each Nₜ trial in 1(A) or 2(B)\n𝐫: 0 (no reward) or 1 (reward) in each Nₜ trial\n\"\"\"\nfunction func_qlearning(init_values, 𝐜, 𝐫) #needed for passing list as variables for Optim\n\n    Nₜ = length(𝐜)\n    Pₐ = zeros(Nₜ) #probabilities of selecting A\n    𝐐 = zeros(Real, (2, Nₜ))\n    logl = 0 #initial value of log likelihood\n\n    for t in 1:Nₜ - 1\n        Pₐ[t] = softmax(init_values[2], 𝐐[1, t] - 𝐐[2, t])\n        logl += (𝐜[t] == 1) * log(Pₐ[t]) + (𝐜[t] == 2) * log(1 - Pₐ[t])\n        𝐐[𝐜[t], t + 1] = 𝐐[𝐜[t], t] + init_values[1] * (𝐫[t] - 𝐐[𝐜[t], t])\n        𝐐[3 - 𝐜[t], t + 1] =  𝐐[3 - 𝐜[t], t]\n    end\n\n    return (negll = -logl, 𝐐 = 𝐐, Pₐ = Pₐ);\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameter Estimation\n\n### optimization with JuMP and Ipopt"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "#if you get no package error for Ipopt, comment out the follwoing lines and run again\n#import Pkg\n#Pkg.add(\"Pkg\")\n#Pkg.add(\"Ipopt\")\n#Pkg.build(\"Ipopt\")\n\nusing JuMP, Ipopt, ForwardDiff\n\n#@manipulate for Nₜ in 0:50:1000, α1 in 0:0.05:1, β1 in 0:0.25:5, Pᵣ in 0:0.05:1\n\n𝐜, 𝐫 = generate_qlearning_data(Nₜ, α1, β1, Pᵣ)\nfunc_qlearning_JuMP(α, β) = func_qlearning((α, β), 𝐜, 𝐫).negll #JuMP requires separate arguments, not a list\n\nm = Model(Ipopt.Optimizer)\nregister(m, :func_qlearning_JuMP, 2, func_qlearning_JuMP, autodiff=true)\n\n@variable(m, 0.0 <= α <= 1.0, start=rand(), base_name = \"learning_rate\")\n@variable(m, 0.0 <= β <= 5.0, start=5*rand(), base_name = \"inverse_temperature\")\n\n@NLobjective(m, Min, func_qlearning_JuMP(α, β))\noptimize!(m)\nprint(\"\",\" α = \", value(α), \" β = \", value(β))\n#end"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### optimization with Optim"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using Optim\n\n@manipulate for Nₜ in 0:5:200, α in 0:0.05:1, β in 0:0.25:5, Pᵣ in 0:0.05:1\n    𝐜, 𝐫 = generate_qlearning_data(Nₜ, α, β, Pᵣ)\n\n    func_qlearning_opt(init_values) = func_qlearning(init_values, 𝐜, 𝐫).negll\n\n    initial_values = rand(2)\n    lower = [0.0, 0.0]\n    upper = [1.0, 5.0]\n    inner_optimizer = GradientDescent()\n    results = optimize(func_qlearning_opt, lower, upper, initial_values, Fminbox(inner_optimizer))\n    #@show optimize(func_qlearning_opt, init_values, lower, upper, LBFGS())\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### optimization with BlackBoxOptim, which is designed for blackbox functions, so this part is only for demonstration purpose"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using BlackBoxOptim\n\n@manipulate for Nₜ in 0:5:200, α in 0:0.05:1, β in 0:0.25:5, Pᵣ in 0:0.05:1\n    𝐜, 𝐫 = generate_qlearning_data(Nₜ, α, β, Pᵣ)\n\n    func_qlearning_opt(init_values) = func_qlearning(init_values, 𝐜, 𝐫).negll\n\n    results = bboptimize(func_qlearning_opt; SearchRange = [(0.0, 1.0), (0.0, 5.0)], NumDimensions = 2);\n    best_candidate(results)\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We can also compare performances when using different optimizers."
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "#this cell takes a lot time to run, so execute it only if you want to\n\n#𝐜, 𝐫 = generate_qlearning_data(100, 0.3, 1.2, 0.5)\n#func_qlearning_opt(init_values) = func_qlearning(init_values, 𝐜, 𝐫).negll\n#compare_optimizers(func_qlearning_opt; SearchRange = [(0.0, 1.0), (0.0, 5.0)], NumDimensions = 2);"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## comparison of models\n\n### win-stay lose-shift (WSLS) model"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"\nNₜ: number of trials\nϵ: error rate\nPᵣ: probability of getting reward in A\n\"\"\"\nfunction wsls_simulstion(Nₜ, ϵ, Pᵣ, seed=1234)\n\n    rng = MersenneTwister(seed)\n\n    Pₐ = zeros(Nₜ) #probabilities of selecting A\n    Pₐ[1] = 0.5 # probability at initial trial is 0.5\n    𝐜 = zeros(Int, Nₜ) #initial choice in each Nₜ trial\n    𝐫 = zeros(Nₜ) # 0 (no reward) or 1 (reward) in each Nₜ trial\n\n    for t in 1:Nₜ-1\n\n        chooseAB = rand(rng)\n        get_reward = rand(rng)\n\n        #select A with reward\n        if chooseAB < Pₐ[t] && get_reward <  Pᵣ\n\n            Pₐ[t + 1] = 1 - ϵ\n            𝐜[t] = 1\n            𝐫[t] = 1\n\n        #select B with no reward\n        elseif chooseAB > Pₐ[t] && get_reward >  Pᵣ\n\n            Pₐ[t + 1] = 1 - ϵ\n            𝐜[t] = 2\n            𝐫[t] = 0\n\n        #select A with no reward\n        elseif chooseAB < Pₐ[t] && get_reward >  Pᵣ\n\n            Pₐ[t + 1] = ϵ\n            𝐜[t] = 1\n            𝐫[t] = 0\n        #select B with reward\n        elseif chooseAB > Pₐ[t] && get_reward <  Pᵣ\n\n            Pₐ[t + 1] = ϵ\n            𝐜[t] = 2\n            𝐫[t] = 1\n\n        end\n\n    end\n\n    return (Pₐ = Pₐ, 𝐜 = 𝐜, 𝐫 = 𝐫);\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### plot"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@manipulate for Nₜ in 0:5:200, ϵ in 0:0.05:1, Pᵣ in 0:0.05:1, seed in 1:1:1234\n\n    Pₐ = wsls_simulstion(Nₜ, ϵ, Pᵣ, seed).Pₐ\n\n    plot(Pₐ, label=\"P(a = A)\", color=\"orange\")\n    ylabel!(\"P(a = A)\")\n    ylims!((0, 1))\n    title!(\"WSLS Model\")\n\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### random selection model"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "function random_choice_simulation(Nₜ, Pₐ, seed=1234)\n\n    rng = MersenneTwister(seed)\n\n    𝐜 = 2 .- Int.(rand(rng, Nₜ) .< Pₐ) #dot notation in Julia signifies elemnet-wise operation\n\n    return (Pₐ = Pₐ, 𝐜 = 𝐜)\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "####plot"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "@manipulate for Nₜ in 0:5:200, Pₐ in 0:0.05:1\n\n    plot([Pₐ for i in range(1, stop=Nₜ)], label=\"P(a = A)\", color=\"orange\")\n    ylabel!(\"P(a = A)\")\n    ylims!((0, 1))\n    title!(\"Random Choice Model\")\n\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### model comparison\n\n#### preparation"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "\"\"\"\nϵ: error rate\n𝐜: vector of choices in each Nₜ trial in 1(A) or 2(B)\n𝐫: 0 (no reward) or 1 (reward) in each Nₜ trial\n\nwhen given ϵ, 𝐜, and 𝐫, returns log likelihood and Pₐ\n\"\"\"\nfunction func_wsls(ϵ, 𝐜, 𝐫)\n\n    Nₜ = length(𝐜)\n    Pₐ = zeros(Nₜ) #probabilities of selecting A\n    Pₐ[1] = 0.5\n    logl = 0 #initial value of log likelihood\n\n    for t in 1:Nₜ - 1\n        logl += (𝐜[t] == 1) * log(Pₐ[t]) + (𝐜[t] == 2) * log(1 - Pₐ[t])\n\n        #select A with reward\n        if 𝐜[t] == 1 &&   𝐫[t] == 1\n\n            Pₐ[t + 1] = 1 - ϵ\n\n        #select B with no reward\n        elseif  𝐜[t] == 2 &&   𝐫[t] == 0\n\n            Pₐ[t + 1] = 1 - ϵ\n\n        #select A with no reward\n        elseif  𝐜[t] == 1 &&   𝐫[t] == 0\n\n            Pₐ[t + 1] = ϵ\n\n        #select B with reward\n        elseif 𝐜[t] == 2 &&   𝐫[t] == 1\n\n            Pₐ[t + 1] = ϵ\n\n        end\n    end\n\n    return (ll = logl, Pₐ = Pₐ);\nend\n\n\n\"\"\"\nPₐ: probability of choosing A\n𝐜: vector of choices in each Nₜ trial in 1(A) or 2(B)\n𝐫: 0 (no reward) or 1 (reward) in each Nₜ trial\n\nwhen given Pₐ, 𝐜, and 𝐫, returns log likelihood and Pₐ\n\"\"\"\nfunction func_random_choice(Pₐ, 𝐜, 𝐫)\n\n    Nₜ = length(𝐜)\n    logl = 0\n\n    for t in 1:Nₜ\n        logl += (𝐜[t] == 1) * log(Pₐ) + (𝐜[t] == 2) * log(1 - Pₐ)\n    end\n\n    return logl\n\nend"
      ],
      "metadata": {},
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### parameter estimation with JuMP"
      ],
      "metadata": {}
    },
    {
      "outputs": [],
      "cell_type": "code",
      "source": [
        "using JuMP, Ipopt, ForwardDiff\n\n@manipulate for Nₜ in 0:50:1000, α1 in 0:0.05:1, β1 in 0:0.25:5, Pᵣ in 0:0.05:1\n\n    𝐜, 𝐫 = generate_qlearning_data(Nₜ, α1, β1, Pᵣ)\n    func_qlearning_JuMP(α, β) = func_qlearning((α, β), 𝐜, 𝐫).negll #JuMP requires separate arguments, not a list\n\n    m = Model(Ipopt.Optimizer)\n    register(m, :func_qlearning_JuMP, 2, func_qlearning_JuMP, autodiff=true)\n\n    @variable(m, 0.0 <= α <= 1.0, start=rand(), base_name = \"learning_rate\")\n    @variable(m, 0.0 <= β <= 5.0, start=5*rand(), base_name = \"inverse_temperature\")\n\n    @NLobjective(m, Min, func_qlearning_JuMP(α, β))\n    optimize!(m)\n    print(\"\",\" α = \", value(α), \" β = \", value(β))"
      ],
      "metadata": {},
      "execution_count": null
    }
  ],
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "file_extension": ".jl",
      "mimetype": "application/julia",
      "name": "julia",
      "version": "1.4.1"
    },
    "kernelspec": {
      "name": "julia-1.4",
      "display_name": "Julia 1.4.1",
      "language": "julia"
    }
  },
  "nbformat": 4
}
