
# Computational Modeling of Behavioral Data by Prof. Kentaro Katahira

## Rescorla-Wagner model

```julia

using Plots
using Interact


"""
Nâ‚œ: number of trials
Î±: learning rate
Páµ£: probability of getting reward
"""

@manipulate for Nâ‚œ = 0:1:500, Î± = 0:0.05:1, Páµ£ = 0:0.05:1

    ğ• = zeros(Nâ‚œ) #strengths of association as Nâ‚œ-length vector
    ğ‘ = rand(Nâ‚œ) .< Páµ£ # presence of reinforcement (1 or 0) as Nâ‚œ-length vector

    for t in 1: Nâ‚œ-1

        ğ•[t+1] = ğ•[t] + Î± *(ğ‘[t]-ğ•[t])
    end

    plot(ğ•, label= string("a ", Î±))
    plot!([(i, Páµ£) for i in 1:1:Nâ‚œ], label="expected value of r: " * string(Páµ£))
    xlabel!("number of trials")
    ylabel!("strength of association")
    ylims!((0, 1))
    title!("Rescorla-Wagner model")
end
```

## Q-learning simulation


### softmax function

```julia

function softmax(Î², Î”q)
    return 1 / (1+ exp(-Î² * (Î”q)))
end

@manipulate for Î² in 0:0.05:5
    plot([(Î”q, softmax(Î², Î”q)) for Î”q in -4:0.1:4], m=:o, label=string("beta ", Î²))
    xlabel!("difference in Q")
    ylabel!("probability")
    ylims!((0, 1))
    title!("Softmax Function")
end
```
### interactive plot of Q-learning model

```julia
# how can I input P with small subscript A?

"""
Nâ‚œ: number of trials
Î±: learning rate
Î²: inverse temperature
Páµ£: probability of getting reward in A
"""

@manipulate for Nâ‚œ in 0:5:200, Î± in 0:0.05:1, Î² in 0:0.25:5, Páµ£ in 0:0.05:1

    ğ = zeros((2, Nâ‚œ)) #initial value of Q in 2 by Nâ‚œ matrix
    ğœ = zeros(Int, Nâ‚œ) #initial choice in each Nâ‚œ trial
    ğ« = zeros(Nâ‚œ) # 0 (no reward) or 1 (reward) in each Nâ‚œ trial
    Pâ‚ = zeros(Nâ‚œ) # probability of choosing A in each trial
    P = (Páµ£, 1-Páµ£)

    for t in 1:Nâ‚œ-1
        Pâ‚ = softmax(Î², ğ[1, t] - ğ[2, t])

        if rand() < Pâ‚
            ğœ[t] = 1 #choose A
            ğ«[t] = Int(rand(Float64) < P[1])
        else
            ğœ[t] = 2 #choose B
            ğ«[t] = Int(rand(Float64) < P[2])
        end

        ğ[ğœ[t], t+1] = ğ[ğœ[t], t] + Î± * (ğ«[t] - ğ[ğœ[t], t])
        ğ[3 - ğœ[t], t+1] = ğ[3 - ğœ[t], t] # retain value of unpicked choice
    end

    plot(ğ[1, :], label="Qt(A)", color="orange")
    plot!([(i, P[1]) for i in 1:1:Nâ‚œ], label="expected value of reward for A:" * string(P[1]), color="darkorange")
    plot!(ğ[2, :], label="Qt(B)", color="skyblue")
    plot!([(i, P[2]) for i in 1:1:Nâ‚œ], label="expected value of reward for B:" * string(P[2]), color="darkblue")
    xlabel!("number of trials")
    ylabel!("Q (value of behavior?)")
    ylims!((0, 1))
    title!("Q-learning model")
end
```

### Optimization with Optim package

```julia

function generate_qlearning_data(Nâ‚œ, Î±, Î², Páµ£)

    ğ = zeros((2, Nâ‚œ)) #initial value of Q in 2 by Nâ‚œ matrix
    ğœ = zeros(Int, Nâ‚œ) #initial choice in each Nâ‚œ trial
    ğ« = zeros(Nâ‚œ) # 0 (no reward) or 1 (reward) in each Nâ‚œ trial
    Pâ‚ = zeros(Nâ‚œ) # probability of choosing A in each trial
    P = (Páµ£, 1-Páµ£)

    for t in 1:Nâ‚œ-1
        Pâ‚ = softmax(Î², ğ[1, t] - ğ[2, t])

        if rand() < Pâ‚
            ğœ[t] = 1 #choose A
            ğ«[t] = (rand() < P[1])
        else
            ğœ[t] = 2 #choose B
            ğ«[t] = Int(rand() < P[2])
        end

        ğ[ğœ[t], t+1] = ğ[ğœ[t], t] + Î± * (ğ«[t] - ğ[ğœ[t], t])
        ğ[3 - ğœ[t], t+1] = ğ[3 - ğœ[t], t] # retain value of unpicked choice
    end

    return ğœ, ğ«
end
```
## Parameter Estimation

```julia
"""
init_values: [Î±, Î²]
Î±: learning rate
Î²: inverse temperature
ğœ: vector of choices in each Nâ‚œ trial in 1(A) or 2(B)
ğ«: 0 (no reward) or 1 (reward) in each Nâ‚œ trial
"""
function func_qlearning(init_values, ğœ, ğ«) #needed for passing list as variables for Optim

    Nâ‚œ = length(ğœ)
    Pâ‚ = zeros(Nâ‚œ) #probabilities of selecting A
    ğ = zeros((2, Nâ‚œ))
    logl = 0 #initial value of log likelihood

    for t in 1:Nâ‚œ - 1
        Pâ‚ = softmax(init_values[2], ğ[1, t] - ğ[2, t])
        logl += (ğœ[t] == 1) * log(Pâ‚) + (ğœ[t] == 2) * log(1 - log(Pâ‚))
        ğ[ğœ[t], t + 1] = ğ[ğœ[t], t] + init_values[1] * (ğ«[t] - ğ[ğœ[t], t])
        ğ[3 - ğœ[t], t + 1] =  ğ[3 - ğœ[t], t]
    end

    return (negll = -logl, ğ = ğ, Pâ‚ = Pâ‚);
end
```

```julia
using Optim

@manipulate for Nâ‚œ in 0:5:200, Î± in 0:0.05:1, Î² in 0:0.25:5, Páµ£ in 0:0.05:1
    ğœ, ğ« = generate_qlearning_data(Nâ‚œ, Î±, Î², Páµ£)

    func_qlearning_opt(init_values) = func_qlearning(init_values, ğœ, ğ«).negll

    initial_values = rand(2)
    lower = [0.0, 0.0]
    upper = [1.0, 5.0]
    inner_optimizer = GradientDescent()
    results = optimize(func_qlearning_opt, lower, upper, initial_values, Fminbox(inner_optimizer))
    #@show optimize(func_qlearning_opt, init_values, lower, upper, LBFGS())
end
```

#### optimization with JuMP and Ipopt
```julia
using JuMP, Ipopt, ForwardDiff
#@manipulate for Nâ‚œ in 0:5:200, Î±1 in 0:0.05:1, Î²1 in 0:0.25:5, Páµ£ in 0:0.05:1

ğœ, ğ« = generate_qlearning_data(50, 0.5, 0.5, 0.5)

func_qlearning_JuMP(Î±, Î²) = func_qlearning((Î±, Î²), ğœ, ğ«).negll

m = Model(Ipopt.Optimizer)
register(m, :func_qlearning_JuMP, 2, func_qlearning_JuMP, autodiff=true)

@variable(m, 0.0 <= Î± <= 1.0, start=rand(), base_name = "learning_rate")
@variable(m, 0.0 <= Î² <= 5.0, start=5*rand(), base_name = "inverse_temperature"))

@NLobjective(m, Min, func_qlearning_JuMP)
optimize!(m)#Thanks Satoshi Terasaki
