# -*- coding: utf-8 -*-
# %% [markdown]
# # Computational Modeling of Behavioral Data by Prof. Kentaro Katahira

# %% [markdown]
# ## Rescorla-Wagner model
#
#
# $$\begin{aligned}
# V_{t + 1} &= V_{t} + \alpha * (r_{t} - V_{t}) \\
# V_t&: strength\ of\ association\ at\ time\ t \\
# \alpha&: learning\ rate \\
# r_t&: reward\ at\ time t \\
# \end{aligned}$$

# %%
using Plots
using Interact
using Random

# %%
"""
Nâ‚œ: number of trials
Î±: learning rate
Páµ£: probability of getting reward
seed: random seed
"""
function plot_rescorla_wagner_model(Nâ‚œ, Î±, Páµ£, seed)
    
    rng = MersenneTwister(seed) # fix random seed

    ğ• = zeros(Nâ‚œ) # initialize strengths of association as Nâ‚œ-length vector
    ğ‘ = rand(rng, Nâ‚œ) .< Páµ£ # presence of reinforcer (1 or 0) as Nâ‚œ-length vector

    for t in 1:Nâ‚œ-1
        ğ•[t+1] = ğ•[t] + Î± * (ğ‘[t] - ğ•[t])
    end

    plot(ğ•, label= string("a ", Î±))
    plot!([(i, Páµ£) for i in 1:1:Nâ‚œ], label="expected value of r: " * string(Páµ£))
    xlabel!("number of trials")
    ylabel!("strength of association")
    ylims!((0, 1))
    title!("Rescorla-Wagner model")
end

# %% [markdown]
# ### Interactive Plot

# %%
@manipulate for Nâ‚œ=0:1:500, Î±=0:0.05:1, Páµ£=0:0.05:1, seed=1:100:1000

    plot_rescorla_wagner_model(Nâ‚œ, Î±, Páµ£, seed)
end

# %% [markdown]
# ## Q-learning simulation
# ### softmax function
#
# $$\begin{aligned}
# P(a_t = A) &= \frac{\exp({\beta*Q_t(A))}}{\exp({\beta*Q_t(A))} + \exp({\beta*Q_t(B))}} \\
# &= \frac{1}{1 + \exp({-\beta*(Q_t(A) - Q_t(B)))}} \\
# &= \frac{1}{1 + \exp({-\beta*(\Delta Q))}}\  (\Delta Q = (Q_t(A) - Q_t(B)))
# \end{aligned}$$

# %%
function softmax(Î², Î”Q)
    return 1 / (1+ exp(-Î² * (Î”Q)))
end

# %% [markdown]
# ### Interactive plot

# %%
@manipulate for Î² in 0:0.05:5
    plot([(Î”q, softmax(Î², Î”q)) for Î”q in -4:0.1:4], m=:o, label=string("beta ", Î²))
    xlabel!("difference in Q")
    ylabel!("probability")
    ylims!((0, 1))
    title!("Softmax Function")
end

# %% [markdown]
# ## Define plot function of Q-learning model

# %%
"""
Nâ‚œ: number of trials
Î±: learning rate
Î²: inverse temperature
Páµ£: probability of getting reward in A
seed: random seed
"""
function plot_q_learning_model(Nâ‚œ, Î±, Î², Páµ£, seed)
    rng = MersenneTwister(seed)

    ğ = zeros(Real, (2, Nâ‚œ)) #initial value of Q in 2 by Nâ‚œ matrix
    ğœ = zeros(Int, Nâ‚œ) #initial choice in each Nâ‚œ trial
    ğ« = zeros(Nâ‚œ) # 0 (no reward) or 1 (reward) in each Nâ‚œ trial
    Pâ‚ = zeros(Nâ‚œ) # probability of choosing A in each trial
    P = (Páµ£, 1-Páµ£)

    for t in 1:Nâ‚œ-1
        Pâ‚ = softmax(Î², ğ[1, t] - ğ[2, t])

        if rand(rng) < Pâ‚
            ğœ[t] = 1 #choose A
            ğ«[t] = Int(rand(rng) < P[1])
        else
            ğœ[t] = 2 #choose B
            ğ«[t] = Int(rand(rng) < P[2])
        end

        ğ[ğœ[t], t+1] = ğ[ğœ[t], t] + Î± * (ğ«[t] - ğ[ğœ[t], t])
        ğ[3 - ğœ[t], t+1] = ğ[3 - ğœ[t], t] # retain value of unpicked choice
    end

    plot(ğ[1, :], label="Qt(A)", color="orange")
    plot!([(i, P[1]) for i in 1:1:Nâ‚œ], label="expected value of reward for A:" * string(P[1]), color="darkorange")
    plot!(ğ[2, :], label="Qt(B)", color="skyblue")
    plot!([(i, P[2]) for i in 1:1:Nâ‚œ], label="expected value of reward for B:" * string(P[2]), color="darkblue")
    xlabel!("number of trials")
    ylabel!("Q (value of behavior?)")
    ylims!((0, 1))
    title!("Q-learning model")
end

# %% [markdown]
# ### Interactive plot

# %%
@manipulate for Nâ‚œ in 0:5:200, Î± in 0:0.05:1, Î² in 0:0.25:5, Páµ£ in 0:0.05:1, seed = 1:1:1000
    plot_q_learning_model(Nâ‚œ, Î±, Î², Páµ£, seed)
end

# %% [markdown]
# ## Parameter Estimation of Q-learing model
# ### Preparation

# %%
function generate_qlearning_data(Nâ‚œ, Î±, Î², Páµ£)

    ğ = zeros(Real, (2, Nâ‚œ)) #initial value of Q in 2 by Nâ‚œ matrix
    ğœ = zeros(Int, Nâ‚œ) #initial choice in each Nâ‚œ trial
    ğ« = zeros(Int, Nâ‚œ) # 0 (no reward) or 1 (reward) in each Nâ‚œ trial
    Pâ‚ = zeros(Real, Nâ‚œ) # probability of choosing A in each trial
    P = (Páµ£, 1-Páµ£)

    for t in 1:Nâ‚œ-1
        Pâ‚ = softmax(Î², ğ[1, t] - ğ[2, t])

        if rand() < Pâ‚
            ğœ[t] = 1 #choose A
            ğ«[t] = (rand() < P[1])
        else
            ğœ[t] = 2 #choose B
            ğ«[t] = Int(rand() < P[2])
        end

        ğ[ğœ[t], t+1] = ğ[ğœ[t], t] + Î± * (ğ«[t] - ğ[ğœ[t], t])
        ğ[3 - ğœ[t], t+1] = ğ[3 - ğœ[t], t] # retain value of unpicked choice
    end

    return ğœ, ğ«
end

# %%
"""
Î±: learning rate
Î²: inverse temperature
ğœ: vector of choices in each Nâ‚œ trial in 1(A) or 2(B)
ğ«: 0 (no reward) or 1 (reward) in each Nâ‚œ trial
"""
function func_qlearning(Î±, Î², ğœ, ğ«)

    Nâ‚œ = length(ğœ)
    Pâ‚ = zeros(Real, Nâ‚œ) #probabilities of selecting A
    ğ = zeros(Real, (2, Nâ‚œ))
    logl = 0 #initial value of log likelihood

    for t in 1:Nâ‚œ - 1
        Pâ‚[t] = softmax(Î², ğ[1, t] - ğ[2, t])
        logl += (ğœ[t] == 1) * log(Pâ‚[t]) + (ğœ[t] == 2) * log(1 - Pâ‚[t])
        ğ[ğœ[t], t + 1] = ğ[ğœ[t], t] + Î± * (ğ«[t] - ğ[ğœ[t], t])
        ğ[3 - ğœ[t], t + 1] =  ğ[3 - ğœ[t], t]
    end

    return (logl = logl, ğ = ğ, Pâ‚ = Pâ‚);
end

# %% [markdown]
# ## Parameter Estimation
# ### optimization with JuMP and Ipopt

# %%
using JuMP, Ipopt, ForwardDiff, Statistics

function estimate_parameter_qlearning(ğœ, ğ«)   
    func_qlearning_JuMP(Î±, Î²) = func_qlearning(Î±, Î², ğœ, ğ«).logl
    
    m = Model(Ipopt.Optimizer)
    register(m, :func_qlearning_JuMP, 2, func_qlearning_JuMP, autodiff=true)

    @variable(m, 0.0 <= Î± <= 1.0, start=rand(), base_name = "learning_rate")
    @variable(m, 0.0 <= Î² <= 5.0, start=rand(), base_name = "inverse_temperature")

    @NLobjective(m, Max, func_qlearning_JuMP(Î±, Î²))
    optimize!(m);
    #print(""," Î± = ", value(Î±), " Î² = ", value(Î²))

    print(""," Î± = ", Î±, " Î² = ", Î²)
    return value(Î±), value(Î²)
end

# %%
Nâ‚œ= 500
Î±1 = 0.3
Î²1 = 0.5
Páµ£ = 0.5
ğœ, ğ« = generate_qlearning_data(Nâ‚œ, Î±1, Î²1, Páµ£)

# %%
estimate_parameter_qlearning(ğœ, ğ«)

# %%

# %% [markdown]
# ## comparison of models
# ### win-stay lose-shift (WSLS) model

# %%
"""
Nâ‚œ: number of trials
Ïµ: error rate
Páµ£: probability of getting reward in A
"""
function wsls_simulation(Nâ‚œ, Ïµ, Páµ£, seed=1234)

    rng = MersenneTwister(seed)

    Pâ‚ = zeros(Nâ‚œ) #probabilities of selecting A
    Pâ‚[1] = 0.5 # probability at initial trial is 0.5
    ğœ = zeros(Int, Nâ‚œ) #initial choice in each Nâ‚œ trial
    ğ« = zeros(Nâ‚œ) # 0 (no reward) or 1 (reward) in each Nâ‚œ trial

    for t in 1:Nâ‚œ-1
        chooseAB = rand(rng)
        get_reward = rand(rng)
        
        #select A with reward
        if chooseAB < Pâ‚[t] && get_reward <  Páµ£
            Pâ‚[t + 1] = 1 - Ïµ
            ğœ[t] = 1
            ğ«[t] = 1

        #select B with no reward
        elseif chooseAB > Pâ‚[t] && get_reward >  Páµ£
            Pâ‚[t + 1] = 1 - Ïµ
            ğœ[t] = 2
            ğ«[t] = 0

        #select A with no reward
        elseif chooseAB < Pâ‚[t] && get_reward >  Páµ£
            Pâ‚[t + 1] = Ïµ
            ğœ[t] = 1
            ğ«[t] = 0
        #select B with reward
        elseif chooseAB > Pâ‚[t] && get_reward <  Páµ£
            Pâ‚[t + 1] = Ïµ
            ğœ[t] = 2
            ğ«[t] = 1
        end
    end
    return (Pâ‚ = Pâ‚, ğœ = ğœ, ğ« = ğ«);
end

# %%
@manipulate for Nâ‚œ in 0:5:200, Ïµ in 0:0.05:1, Páµ£ in 0:0.05:1, seed in 1:1:1234

    Pâ‚ = wsls_simulation(Nâ‚œ, Ïµ, Páµ£, seed).Pâ‚

    plot(Pâ‚, label="P(a = A)", color="orange")
    ylabel!("P(a = A)")
    ylims!((0, 1))
    title!("WSLS Model")

end

# %% [markdown]
# ### random selection model

# %%
function random_choice_simulation(Nâ‚œ, Pâ‚, seed=1234)

    rng = MersenneTwister(seed)

    ğœ = 2 .- Int.(rand(rng, Nâ‚œ) .< Pâ‚) #dot notation in Julia signifies elemnet-wise operation

    return (Pâ‚ = Pâ‚, ğœ = ğœ)
end

# %% [markdown]
# ### Interactive plot

# %%
@manipulate for Nâ‚œ in 0:5:200, Pâ‚ in 0:0.05:1

    plot([Pâ‚ for i in range(1, stop=Nâ‚œ)], label="P(a = A)", color="orange")
    ylabel!("P(a = A)")
    ylims!((0, 1))
    title!("Random Choice Model")

end

# %% [markdown]
# ### model comparison
# #### preparation

# %%
"""
Ïµ: error rate
ğœ: vector of choices in each Nâ‚œ trial in 1(A) or 2(B)
ğ«: 0 (no reward) or 1 (reward) in each Nâ‚œ trial

when given Ïµ, ğœ, and ğ«, returns log likelihood and Pâ‚
"""
function func_wsls(Ïµ, ğœ, ğ«)

    Nâ‚œ = length(ğœ)
    Pâ‚ = zeros(Nâ‚œ) #probabilities of selecting A
    Pâ‚[1] = 0.5
    logl = 0.0 #initial value of log likelihood

    for t in 1:Nâ‚œ - 1
        logl += (ğœ[t] == 1) * log(Pâ‚[t]) + (ğœ[t] == 2) * log(1 - Pâ‚[t])

        #select A with reward
        if ğœ[t] == 1 &&   ğ«[t] == 1
            Pâ‚[t + 1] = 1 - Ïµ

        #select B with no reward
        elseif  ğœ[t] == 2 &&   ğ«[t] == 0
            Pâ‚[t + 1] = 1 - Ïµ

        #select A with no reward
        elseif  ğœ[t] == 1 &&   ğ«[t] == 0
            Pâ‚[t + 1] = Ïµ

        #select B with reward
        elseif ğœ[t] == 2 &&   ğ«[t] == 1
            Pâ‚[t + 1] = Ïµ

        end
    end

    return (nlogl = -logl, Pâ‚ = Pâ‚);
end

# %%
using Optim

function estimate_parameter_wsls(ğœ, ğ«)   

    func_wsls_opt(Ïµ) = func_wsls(Ïµ, ğœ, ğ«).logl # why can't I use nlogl?

    result = optimize(func_wsls_opt, 0.0, 1.0)
    print(result)
    return Optim.minimizer(result)
end

# %%
estimate_parameter_wsls(ğœ, ğ«)

# %%
wsls_data = wsls_simulation(200, 0.2, 0.5, 1234) 
ğœ, ğ«  = wsls_data[1], wsls_data[2]
estimate_parameter_wsls(ğœ, ğ«)

# %%
using Plots
plot([0:0.01:1], [func_wsls(i, ğœ, ğ«).logl for i in 0:0.01:1])
# %%
"""
Pâ‚: probability of choosing A
ğœ: vector of choices in each Nâ‚œ trial in 1(A) or 2(B)
ğ«: 0 (no reward) or 1 (reward) in each Nâ‚œ trial

when given Pâ‚, ğœ, and ğ«, returns log likelihood and Pâ‚
"""
function func_random_choice(Pâ‚, ğœ, ğ«)

    Nâ‚œ = length(ğœ)
    logl = 0

    for t in 1:Nâ‚œ
        logl += (ğœ[t] == 1) * log(Pâ‚) + (ğœ[t] == 2) * log(1 - Pâ‚)
    end

    return logl

end

# %% [markdown]
# #### compare log-likelihood and AIC of each model

# %%
Nâ‚œ= 500
Î±1 = 0.3
Î²1 = 2
Páµ£ = 0.5

ğœ, ğ« = generate_qlearning_data(Nâ‚œ, Î±1, Î²1, Páµ£)

# Q-learning model
Î±, Î² = estimate_parameter_qlearning(ğœ, ğ«)


# %%

# %%

# %%

# %%
